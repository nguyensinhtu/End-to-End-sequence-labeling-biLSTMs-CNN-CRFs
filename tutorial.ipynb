{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datapreprocessing <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "#params\n",
    "params = {}\n",
    "params['batch_size'] = 20\n",
    "params['words'] = '../Data/words.txt'\n",
    "params['tags'] = '../Data/tags.txt'\n",
    "params['shuffle'] = False\n",
    "params['batch_size'] = 2\n",
    "params['prefech_size'] = 0\n",
    "params['buffer_size'] = 5\n",
    "params['epochs'] = 2\n",
    "#sử dụng eager execution chúng ta không phải define một static graph\n",
    "tf.enable_eager_execution()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data sử dụng tf.data dể feed data vào model <br>\n",
    "Sử dụng tf.data để loadfile, sử dụng tf data là vì nó khắc phục tình trạng model bị đói data (data starving), thích hợp khi train sử dụng GPU <br> <br>\n",
    "\n",
    "Ta sử dụng tf.data.Dataset.from_generator để load file, cho phép ta thực hiện một số bước tiền xử lý trước khi đưa vào model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trích xuất character level cho mô hình cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_character(token, default_value='<pad_char>'):\n",
    "    # split token, tf.string_split trả lại dense vector\n",
    "    split_char = tf.string_split(token, delimiter='')\n",
    "    \n",
    "    # convert sparse vector to dense vector\n",
    "    split_char = tf.sparse.to_dense(split_char, default_value=default_value)\n",
    "    \n",
    "    return split_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([b'I'], 1), [b'O'], <tf.Tensor: id=1560, shape=(1, 1), dtype=string, numpy=array([[b'I']], dtype=object)>)\n",
      "(([b'I', b'live', b'in', b'San', b'Francisco'], 5), [b'O', b'O', b'O', b'B-LOC', b'I-LOC'], <tf.Tensor: id=1568, shape=(5, 9), dtype=string, numpy=\n",
      "array([[b'I', b'<pad_char>', b'<pad_char>', b'<pad_char>', b'<pad_char>',\n",
      "        b'<pad_char>', b'<pad_char>', b'<pad_char>', b'<pad_char>'],\n",
      "       [b'l', b'i', b'v', b'e', b'<pad_char>', b'<pad_char>',\n",
      "        b'<pad_char>', b'<pad_char>', b'<pad_char>'],\n",
      "       [b'i', b'n', b'<pad_char>', b'<pad_char>', b'<pad_char>',\n",
      "        b'<pad_char>', b'<pad_char>', b'<pad_char>', b'<pad_char>'],\n",
      "       [b'S', b'a', b'n', b'<pad_char>', b'<pad_char>', b'<pad_char>',\n",
      "        b'<pad_char>', b'<pad_char>', b'<pad_char>'],\n",
      "       [b'F', b'r', b'a', b'n', b'c', b'i', b's', b'c', b'o']],\n",
      "      dtype=object)>)\n",
      "(([b'I', b'live'], 2), [b'O', b'O'], <tf.Tensor: id=1576, shape=(2, 4), dtype=string, numpy=\n",
      "array([[b'I', b'<pad_char>', b'<pad_char>', b'<pad_char>'],\n",
      "       [b'l', b'i', b'v', b'e']], dtype=object)>)\n",
      "(([b'You', b'live', b'in', b'Paris'], 4), [b'O', b'O', b'O', b'S-LOC'], <tf.Tensor: id=1584, shape=(4, 5), dtype=string, numpy=\n",
      "array([[b'Y', b'o', b'u', b'<pad_char>', b'<pad_char>'],\n",
      "       [b'l', b'i', b'v', b'e', b'<pad_char>'],\n",
      "       [b'i', b'n', b'<pad_char>', b'<pad_char>', b'<pad_char>'],\n",
      "       [b'P', b'a', b'r', b'i', b's']], dtype=object)>)\n"
     ]
    }
   ],
   "source": [
    "def generator_fn(words, tags):\n",
    "    \"\"\"\n",
    "    input : đường dẫn words (từng dòng chứa token), tags(từng dòng chữa nhãn)\n",
    "    \"\"\"\n",
    "    with Path(words).open('r') as f_words, Path(tags).open('r') as f_tags:\n",
    "        for line_words, line_tags in zip(f_words, f_tags):\n",
    "            # encode word, tag trước khi đưa vào model\n",
    "            list_words = [w.encode() for w in line_words.strip().split()]\n",
    "            list_tags = [t.encode() for t in line_tags.strip().split()]\n",
    "            list_char_of_words = extract_character(list_words)\n",
    "            # khi gọi hàm yeild sẽ trả về đối tượng của hàm này, và khi nào thực hiện\n",
    "            # lặp nó sẽ sinh ra data khi gặp yeild và không dữ lại trên ram\n",
    "            \n",
    "            assert len(list_words) == len(list_tags)\n",
    "            \n",
    "            yield (list_words, len(list_words)), list_tags, list_char_of_words\n",
    "            \n",
    "gc = generator_fn(params['words'], params['tags'])\n",
    "for i in gc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load word embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeding():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contruct input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(words, tags, shuffle = False):\n",
    "    # định nghĩa kiểu data\n",
    "    types = ((tf.string, tf.int32), tf.string)\n",
    "    \n",
    "    #định nghĩa ouput shape của data\n",
    "    output_shape = (([None], ()), [None], tf.TensorShape([None]))\n",
    "    \n",
    "    # do ta phải pass một hàm vào và hàm generator_fn có tham số nên dể pass \n",
    "    # tham số ta dùng functools.partital\n",
    "    dataset = tf.data.Dataset.from_generator(functools.partial(generator_fn, words, tags)\n",
    "                                                       , types, \\\n",
    "                                             output_shape)\n",
    "    \n",
    "    # shuffle data, buffer_size ở đây là số phần tử tensor sẽ lấy vào buffer\n",
    "    # và thực hiện shuffle, nếu data ở riêng từng lớp nên chọn buffer_size = số sample trong toàn data\n",
    "    # sau dó thực hiện repeat lại suffle này cho các epochs khác\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size = params['buffer_size']).repeat(count = params['epochs'])\n",
    "    \n",
    "    # giá trị padding thêm vào những câu ngắn sao cho chiều dài các câu bằng chiều\n",
    "    # chiều dài câu lớn nhất trong batch\n",
    "    default_value = (('<pad>', 0), 'O', ('<pad_char>'))\n",
    "    \n",
    "    # tạo batch\n",
    "    #prefech là số batch mà CPU sẽ load sẵn lên ram cho GPU\n",
    "    dataset = dataset.padded_batch(params['batch_size'], output_shape, default_value) \\\n",
    "    .prefetch(params['prefech_size'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same sequence length. Input structure has length 3, while shallow structure has length 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-9be86ff39cf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tags'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-08f582533911>\u001b[0m in \u001b[0;36minput_fn\u001b[1;34m(words, tags, shuffle)\u001b[0m\n\u001b[0;32m     10\u001b[0m     dataset = tf.data.Dataset.from_generator(functools.partial(generator_fn, words, tags)\n\u001b[0;32m     11\u001b[0m                                                        \u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                                              output_shape)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# shuffle data, buffer_size ở đây là số phần tử tensor sẽ lấy vào buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_generator\u001b[1;34m(generator, output_types, output_shapes, args)\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m       output_shapes = nest.map_structure_up_to(\n\u001b[1;32m--> 406\u001b[1;33m           output_types, tensor_shape.as_shape, output_shapes)\n\u001b[0m\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m       \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure_up_to\u001b[1;34m(shallow_tree, func, *inputs)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot map over no sequences\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0minput_tree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m     \u001b[0massert_shallow_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m   \u001b[1;31m# Flatten each input separately, apply the function to corresponding elements,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\u001b[0m in \u001b[0;36massert_shallow_structure\u001b[1;34m(shallow_tree, input_tree, check_types)\u001b[0m\n\u001b[0;32m    311\u001b[0m           \u001b[1;34m\"The two structures don't have the same sequence length. Input \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m           \u001b[1;34m\"structure has length %s, while shallow structure has length %s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m           % (len(input_tree), len(shallow_tree)))\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_types\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The two structures don't have the same sequence length. Input structure has length 3, while shallow structure has length 2."
     ]
    }
   ],
   "source": [
    "dataset = input_fn(params['words'], params['tags'])\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "node = iterator.get_next()\n",
    "print(node)\n",
    "node = iterator.get_next()\n",
    "print (node)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs - LSTM - CRF mode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN model cho character level embeding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "def f1(i):\n",
    "    w = []\n",
    "    w.append(i)\n",
    "    return w\n",
    "\n",
    "def createGenerator():\n",
    "    mylist = range(3)\n",
    "    for i in mylist:\n",
    "        yield f1(i)\n",
    "gc = createGenerator()\n",
    "for i in gc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
